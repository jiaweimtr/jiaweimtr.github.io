<!DOCTYPE html>
<!--[if lt IE 8 ]><html class="no-js ie ie7" lang="en"> <![endif]-->
<!--[if IE 8 ]><html class="no-js ie ie8" lang="en"> <![endif]-->
<!--[if IE 9 ]><html class="no-js ie ie9" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!--><html class="no-js" lang="en"> <!--<![endif]-->
<head>

   <!--- Basic Page Needs
   ================================================== -->
   <meta charset="utf-8">
	<title>Blog - Implementing Backpropagation</title>
	<meta name="description" content="">  
	<meta name="author" content="">
        
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});
    </script>
    <script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

   <!-- Mobile Specific Metas
   ================================================== -->
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<!-- CSS
    ================================================== -->
   <link rel="stylesheet" href="css/base.css">
	<link rel="stylesheet" href="css/main.css">
   <link rel="stylesheet" href="css/media-queries.css">         

   <!-- Script
   =================================================== -->
	<script src="js/modernizr.js"></script>

   <!-- Favicons
	=================================================== -->
	<link rel="shortcut icon" href="favicon.png" >

</head>

<body>

	<div id="top"></div>
   <div id="preloader"> 
	   <div id="status">
         <img src="images/loader.gif" height="60" width="60" alt="">
         <div class="loader">Loading...</div>
      </div>
   </div>

   <!-- Header
   =================================================== -->
   <header id="main-header">

   	<div class="row header-inner">

	      <div class="logo">
	         <a class="smoothscroll" </a>
	      </div>

	      <nav id="nav-wrap">         
	         
	         <a class="mobile-btn" href="#nav-wrap" title="Show navigation">
	         	<span class='menu-text'>Show Menu</span>
	         	<span class="menu-icon"></span>
	         </a>
         	<a class="mobile-btn" href="#" title="Hide navigation">
         		<span class='menu-text'>Hide Menu</span>
         		<span class="menu-icon"></span>
         	</a>         

	         <ul id="nav" class="nav">
                 <li><a href="index.html#about">About</a></li>
		         <li><a href="index.html#portfolio">Research</a></li>
	            <li class="current"><a href="index.html#journal">Blog</a></li>
	            <li><a href="index.html#contact">Contact</a></li>
	         </ul> 

	      </nav> <!-- /nav-wrap -->	      

	   </div> <!-- /header-inner -->

   </header>


   <!-- Page Title
   ================================================== -->
   <section id="page-title">	
   	  
		<div class="row">

			<div class="twelve columns">

				<h1>Blog<span>.</span></h1>

			</div>			    

		</div> <!-- /row -->	   

   </section> <!-- /page-title -->


   <!-- Content
   ================================================== -->
   <section id="content">

   	<div class="row">

	   	<div id="main" class="tab-whole eight columns">

	         <article class="entry">

					<header class="entry-header">

						<h1 class="entry-title">
							Implementing Backpropagation
						</h1> 				 
						
						<div class="entry-meta">
							<ul>
								<li>March 29, 2016</li>
								<span class="meta-sep">â€¢</span>
								<li>Joe Marino</li>
							</ul>
						</div> 
						 
					</header>

					<div class="entry-content-media">
						<div class="post-thumb">
							<img src="images/neural_net.jpg">
						</div> 
					</div> 
						
					<div class="entry-content">
                        <p class="lead">Or, how to train your neural network.</p>

                        <p>
                        This past fall, I had the pleasure of TAing Caltech's course on neural computation, CNS 187. The course provides an overview of a number of biologically-inspired methods of computation, many of which are taken from the machine learning literature. The pinnacle of the class is the dreaded <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">multilayer perceptron</a>, a standard feedforward artificial neural network. Students are required to implement a general neural network, including the <a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a> algorithm, which is used to train the network. Having had to derive the algorithm a number of times for my students, the details are still relatively fresh in my mind, so I share them here for any future students who may be struggling with the material. By the end of this blog post, you will know how to implement the backpropagation algorithm.
                        </p>
                        
                        <p>
                        Before we get into things, relax, take a deep breath, and remember the following: at the end of the day, <b>feedforward neural networks are just functions</b>, mapping inputs $x$ to output predictions $\hat{y}$:
                        
                        $$\hat{y} = f(x).$$
                        
                        This function could be something simple, such as the identity function $f(x) = x$, or it could be incredibly intricate, like mapping pixel intensities to an object. The required complexity of the function depends on the complexity of the mapping between $x$ and the ground truth label $y$, our desired output; if we want to solve a difficult problem, we need a function that is well-suited (i.e. with enough parameters) to handle it. Behind all of the notation and equations, <b>backpropagation is simply an algorithm for learning the function $f$ from a set of data examples of the form $(x,y)$.</b>
                        </p>
                        
                        <p>
                        Feedforward neural networks are functions of a very specific form. They contain many units, analogous to biological neurons, each performing the same set of operation: a linear summation and a non-linearity. Let's break each of these down. What is a linear summation? Say we have a vector of inputs, $\vec{x}$, and say we have a corresponding vector of weights (with the same number of entries as $\vec{x}$), $\vec{w}$. A linear summation has the form
                        
                        \begin{equation}
                            \sum_i w_i x_i,
                        \end{equation}
                            
                            where $i$ indexes over the entries in $w$ and $x$. The result of this operation is a scalar. If you are familiar with basic linear algebra, this can be equivalently written as the dot product of these two vectors:
                            
                            $$\vec{w} \cdot \vec{x}.$$
                            
                            The 'linear' part comes from the fact that we are multiplying each weight by a linear version of the input. That is to say
                        </p>
                        
                        <p>
                        We have a feedforward neural network, as shown in the following figure:
                        </p>
                        
                        <center>
                            <img src="images/neural_net.png">
                        </center>
                        
						<p>
                        At each layer, we have a set of units $x$, a set of activations $s$, and a set of weights $W$. There are a total of $L$ layers in the network. We feed our data into layer $0$ and we have some output at layer $L$. At each layer, we include an additional unit called the bias (the dotted outlined unit), that is arbitrarily set to 1. Why do we do this, you ask? Let's take a look at each layer's computations to find out. $s^{\ell}_j$, the $j^{th}$ activation at layer $\ell$, is found by taking a weighted sum of the previous layer's units and adding on a bias term:
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        To run the data through the network, we perform a series of computations at each layer. These computations consist of a linear weighted sum of the previous layer's units to get the current layer's activations:
                        
                        $$s^\ell = W^\ell \begin{pmatrix} 1 \\ x^{\ell-1} \end{pmatrix}.$$
                        
                        The $1$ is the bias unit. This equation is in a <em> vectorized </em> form, meaning that $s, W,$ and $x$ are all either vectors or matrices. This is followed by a non-linearity to get the current layer's units:
                        
                        $$x^\ell = g(s^\ell).$$
                        
                        $g( )$ is a monotonic, differentiable non-linearity function. For instance, we can use a rectified linear unit (ReLU), hyperbolic tangent (tanh), logistic sigmoid, or others. As with most other introductory tutorials to backpropagation, I will use the logistic sigmoid non-linearity, which has the form
                        
                        $$ g(s) = \frac{1}{1+e^{-s}},$$
                        
                        and looks something like:
                        <center>
                            <img src="images/sigmoid.png">
                        </center>
                        
                        Feedforward neural networks are most commonly use in a supervised learning setting.
                        
						</p>

					</div>
                    
                    <div id="disqus_thread"></div>
                    <script>
                        /**
                         * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
                         * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
                         */
                    
                    var disqus_config = function () {
                        this.page.url = 'http://joelouismarino.github.io/blog_backprop.html'; // Replace PAGE_URL with your page's canonical URL variable
                        this.page.identifier = implementing_backprop; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
                    };
                    
                    (function() { // DON'T EDIT BELOW THIS LINE
                     var d = document, s = d.createElement('script');
                     
                     s.src = '//marinoblog.disqus.com/embed.js';
                     
                     s.setAttribute('data-timestamp', +new Date());
                     (d.head || d.body).appendChild(s);
                     })();
                        </script>
                    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

				</article> <!-- /entry -->
						   
	         
	   	</div> <!-- /main -->

	   </div> <!-- /row -->      

   </section> <!-- /content -->  


<!-- Footer
 ================================================== -->
<footer>
    
    <div class="row">
        
        <div class="twelve columns tab-whole right-cols">
            
            <div class="row">
                
                <div class="columns">
                    <h3 class="address">Contact</h3>
                    <p>
                    jmarino [at] caltech.edu
                    </p>
                    <p>
                    1200 E. California Blvd.<br>
                    MC 136-93<br>
                    Pasadena, 91125 CA<br>
                    </p>
                    <p>
                    651.468.6441
                    </p>
                </div> <!-- /columns -->
                
                <div class="columns">
                    <h3 class="contact">Connect</h3>
                    
                    <ul>
                        <li><a href="https://www.linkedin.com/pub/joe-marino/37/bb0/73"><span></span><i class="fa fa-linkedin"></i></a>
                            <a href="https://github.com/joelouismarino"><span></span><i class="fa fa-github"></i></a>
                            <a href="https://www.facebook.com/joe.marino.9022"><span></span><i class="fa fa-facebook"></i></a>
                        </li>
                    </ul>
                    
                </div> <!-- /Row(nested) -->
                
            </div>
            
            <p class="copyright">&copy; Copyright 2015 Joseph Marino.</a></p>
            
            <div id="go-top">
                <a class="smoothscroll" title="Back to Top" href="#top"><span>Top</span><i class="fa fa-long-arrow-up"></i></a>
            </div>
            
        </div> <!-- /row -->
        
        </footer> <!-- /footer -->


   <!-- Java Script
   ================================================== -->
   <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
   <script>window.jQuery || document.write('<script src="js/jquery-1.10.2.min.js"><\/script>')</script>
   <script type="text/javascript" src="js/jquery-migrate-1.2.1.min.js"></script>   
   <script src="js/jquery.flexslider.js"></script>
   <script src="js/jquery.fittext.js"></script>
   <script src="js/backstretch.js"></script> 
   <script src="js/waypoints.js"></script>  
   <script src="js/main.js"></script>
   <script id="dsq-count-scr" src="//marinoblog.disqus.com/count.js" async></script>

</body>

</html>