<!DOCTYPE html>
<!--[if lt IE 8 ]><html class="no-js ie ie7" lang="en"> <![endif]-->
<!--[if IE 8 ]><html class="no-js ie ie8" lang="en"> <![endif]-->
<!--[if IE 9 ]><html class="no-js ie ie9" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!--><html class="no-js" lang="en"> <!--<![endif]-->
<head>

   <!--- Basic Page Needs
   ================================================== -->
   <meta charset="utf-8">
	<title>Blog - Implementing Backpropagation</title>
	<meta name="description" content="">  
	<meta name="author" content="">
        
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});
    </script>
    <script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

   <!-- Mobile Specific Metas
   ================================================== -->
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<!-- CSS
    ================================================== -->
   <link rel="stylesheet" href="css/base.css">
	<link rel="stylesheet" href="css/main.css">
   <link rel="stylesheet" href="css/media-queries.css">         

   <!-- Script
   =================================================== -->
	<script src="js/modernizr.js"></script>

   <!-- Favicons
	=================================================== -->
	<link rel="shortcut icon" href="favicon.png" >

</head>

<body>

	<div id="top"></div>
   <div id="preloader"> 
	   <div id="status">
         <img src="images/loader.gif" height="60" width="60" alt="">
         <div class="loader">Loading...</div>
      </div>
   </div>

   <!-- Header
   =================================================== -->
   <header id="main-header">

   	<div class="row header-inner">

	      <div class="logo">
	         <a class="smoothscroll" </a>
	      </div>

	      <nav id="nav-wrap">         
	         
	         <a class="mobile-btn" href="#nav-wrap" title="Show navigation">
	         	<span class='menu-text'>Show Menu</span>
	         	<span class="menu-icon"></span>
	         </a>
         	<a class="mobile-btn" href="#" title="Hide navigation">
         		<span class='menu-text'>Hide Menu</span>
         		<span class="menu-icon"></span>
         	</a>         

	         <ul id="nav" class="nav">
                 <li><a href="index.html#about">About</a></li>
		         <li><a href="index.html#portfolio">Research</a></li>
	            <li class="current"><a href="index.html#journal">Blog</a></li>
	            <li><a href="index.html#contact">Contact</a></li>
	         </ul> 

	      </nav> <!-- /nav-wrap -->	      

	   </div> <!-- /header-inner -->

   </header>


   <!-- Page Title
   ================================================== -->
   <section id="page-title">	
   	  
		<div class="row">

			<div class="twelve columns">

				<h1>Blog<span>.</span></h1>

			</div>			    

		</div> <!-- /row -->	   

   </section> <!-- /page-title -->


   <!-- Content
   ================================================== -->
   <section id="content">

   	<div class="row">

	   	<div id="main" class="tab-whole eight columns">

	         <article class="entry">

					<header class="entry-header">

						<h1 class="entry-title">
							Implementing Backpropagation
						</h1> 				 
						
						<div class="entry-meta">
							<ul>
								<li>March 29, 2016</li>
								<span class="meta-sep">â€¢</span>
								<li>Joe Marino</li>
							</ul>
						</div> 
						 
					</header>

					<div class="entry-content-media">
						<div class="post-thumb">
							<img src="images/neural_net.jpg">
						</div> 
					</div> 
						
					<div class="entry-content">
                        <p class="lead">Or, how to train your neural network.</p>

                        <p>
                        This past fall, I had the pleasure of TAing Caltech's course on neural computation, CNS 187. The course provides an overview of a number of biologically-inspired methods of computation, many of which are taken from the machine learning literature. The pinnacle of the class is the dreaded <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">multilayer perceptron</a>, a standard feedforward artificial neural network. Students are required to implement a general neural network, including the <a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a> algorithm, which is used to train the network. Having had to derive the algorithm a number of times for my students, the details are still relatively fresh in my mind, so I share them here for any future students who may be struggling with the material. By the end of this blog post, you will know how to implement the backpropagation algorithm.
                        </p>
                        
                        <p>
                        Before we get into things, relax, take a deep breath, and remember the following: at the end of the day, <b>feedforward neural networks are just functions</b>, mapping inputs $x$ to output predictions $\hat{y}$:
                        
                        \begin{equation}
                            \hat{y} = f(x).
                        \end{equation}
                        
                        This function could be something simple, such as the identity function $f(x) = x$, or it could be incredibly intricate, like mapping pixel intensities to an object. The required complexity of the function depends on the complexity of the mapping between $x$ and the ground truth label $y$, our desired output; if we want to solve a difficult problem, we need a function that is well-suited (i.e. with enough parameters) to handle it. Behind all of the notation and equations, <b>backpropagation is simply an algorithm for learning the function $f$ from a set of data examples of the form $(x,y)$.</b>
                        </p>
                        
                        <h3>Linear Summation + Non-Linear Transformation</h3>
                        
                        <p>
                        Feedforward neural networks are functions of a very specific form. They contain many units, analogous to biological neurons, each performing the same set of operations: a linear summation and a non-linear transformation. Let's break each of these down. What is a linear summation? Say we have a vector of inputs, $\vec{x}$, and say we have a corresponding vector of weights, $\vec{w}$, with the same number of entries as $\vec{x}$. A linear summation has the form
                        
                        \begin{equation}
                            \sum_i w_i x_i,
                        \end{equation}
                            
                            where $i$ indexes over the entries in $w$ and $x$. The result of this operation is a scalar. If you are familiar with basic linear algebra, this can be equivalently written as the <a href="https://en.wikipedia.org/wiki/Dot_product">dot product</a> of these two vectors:
                            
                        \begin{equation}
                            \vec{w} \cdot \vec{x}.
                        \end{equation}
                        
                            The <em>linear</em> part comes from the fact that we are multiplying each weight by a linear version of the input. That is to say, in (2), we multiply each $w_i$ by $x_i$, not by $x_i^2$ or $\log x_i$ or something else. The <em>summation</em> part is straightfoward: we sum up the results from each of these multiplications.
                        </p>
                        <p>
                        Now on to the non-linear transformation. After taking a linear summation of the inputs, we pass the resulting scalar through a non-linear function. While there are many options that will work, the non-linearity traditionally employed in the neural network literature is the logistic <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid function</a>, which has the form
                        
                        \begin{equation}
                            g(s) = \frac{1}{1+e^{-s}}.
                        \end{equation}
                        
                        Plotting this out, we get the following curve:
                        
                        <center>
                            <img src="images/sigmoid_labels.png">
                        </center>
                        
                        For large negative inputs, the exponential blows up and the function approaches 0, whereas for large positive inputs, the exponential goes to zero and the function approaches 1. For inputs that lie between these two extremes, we fall into an approximately linear region.
                        </p>
                        <p>
                        Let's look at another possible non-linearity, the <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">rectified linear unit (ReLU)</a>. This function has the form
                        
                        \begin{equation}
                            g(s) = \text{max}(0,s).
                        \end{equation}
                        
                        When plotted out, we have this curve:
                        
                        <center>
                            <img src="images/relu_labels.png">
                        </center>
                        
                        Anything less than zero gets mapped to zero, while anything greater than zero is passed through as the identity.
                        
                        </p>
                        <p>
                        There are arguments in favor of using each of these non-linearities. However, in practice, using the right training techniques, it may not make a substantial difference which non-linear function we choose for our network. We are also not constrained to use the same non-linearity for every unit in the network. We only require that whichever non-linear function we choose be 1) <a href="https://en.wikipedia.org/wiki/Monotonic_function">monotonic</a> and 2) differentiable and defined everywhere. The second point is actually not entirely strict, since we can get by with using the ReLU function, even though it has a (non-differentiable) cusp at 0.
                        </p>
                        
                        <p>
                        Stepping back, let's ask ourselves: why a linear summation followed by a non-linear transform? The motivation is rooted in biological neurons. Neurons receive inputs from other neurons through branching structures called dendrites.
                        </p>
                        <p>
                        We have a feedforward neural network, as shown in the following figure:
                        </p>
                        
                        <center>
                            <img src="images/neural_net.png">
                        </center>
                        
						<p>
                        At each layer, we have a set of units $x$, a set of activations $s$, and a set of weights $W$. There are a total of $L$ layers in the network. We feed our data into layer $0$ and we have some output at layer $L$. At each layer, we include an additional unit called the bias (the dotted outlined unit), that is arbitrarily set to 1. Why do we do this, you ask? Let's take a look at each layer's computations to find out. $s^{\ell}_j$, the $j^{th}$ activation at layer $\ell$, is found by taking a weighted sum of the previous layer's units and adding on a bias term:
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        To run the data through the network, we perform a series of computations at each layer. These computations consist of a linear weighted sum of the previous layer's units to get the current layer's activations:
                        
                        $$s^\ell = W^\ell \begin{pmatrix} 1 \\ x^{\ell-1} \end{pmatrix}.$$
                        
                        The $1$ is the bias unit. This equation is in a <em> vectorized </em> form, meaning that $s, W,$ and $x$ are all either vectors or matrices. This is followed by a non-linearity to get the current layer's units:
                        
                        $$x^\ell = g(s^\ell).$$
                        
                        $g( )$ is a monotonic, differentiable non-linearity function. For instance, we can use a rectified linear unit (ReLU), hyperbolic tangent (tanh), logistic sigmoid, or others. As with most other introductory tutorials to backpropagation, I will use the logistic sigmoid non-linearity, which has the form
                        
                        $$ g(s) = \frac{1}{1+e^{-s}},$$
                        
                        and looks something like:
                        <center>
                            <img src="images/sigmoid.png">
                        </center>
                        
                        Feedforward neural networks are most commonly use in a supervised learning setting.
                        
						</p>

					</div>
                    
                    <div id="disqus_thread"></div>
                    <script>
                        /**
                         * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
                         * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
                         */
                    
                    var disqus_config = function () {
                        this.page.url = 'http://joelouismarino.github.io/blog_backprop.html'; // Replace PAGE_URL with your page's canonical URL variable
                        this.page.identifier = implementing_backprop; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
                    };
                    
                    (function() { // DON'T EDIT BELOW THIS LINE
                     var d = document, s = d.createElement('script');
                     
                     s.src = '//marinoblog.disqus.com/embed.js';
                     
                     s.setAttribute('data-timestamp', +new Date());
                     (d.head || d.body).appendChild(s);
                     })();
                        </script>
                    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

				</article> <!-- /entry -->
						   
	         
	   	</div> <!-- /main -->

	   </div> <!-- /row -->      

   </section> <!-- /content -->  


<!-- Footer
 ================================================== -->
<footer>
    
    <div class="row">
        
        <div class="twelve columns tab-whole right-cols">
            
            <div class="row">
                
                <div class="columns">
                    <h3 class="address">Contact</h3>
                    <p>
                    jmarino [at] caltech.edu
                    </p>
                    <p>
                    1200 E. California Blvd.<br>
                    MC 136-93<br>
                    Pasadena, 91125 CA<br>
                    </p>
                    <p>
                    651.468.6441
                    </p>
                </div> <!-- /columns -->
                
                <div class="columns">
                    <h3 class="contact">Connect</h3>
                    
                    <ul>
                        <li><a href="https://www.linkedin.com/pub/joe-marino/37/bb0/73"><span></span><i class="fa fa-linkedin"></i></a>
                            <a href="https://github.com/joelouismarino"><span></span><i class="fa fa-github"></i></a>
                            <a href="https://www.facebook.com/joe.marino.9022"><span></span><i class="fa fa-facebook"></i></a>
                        </li>
                    </ul>
                    
                </div> <!-- /Row(nested) -->
                
            </div>
            
            <p class="copyright">&copy; Copyright 2016 Joseph Marino.</a></p>
            
            <div id="go-top">
                <a class="smoothscroll" title="Back to Top" href="#top"><span>Top</span><i class="fa fa-long-arrow-up"></i></a>
            </div>
            
        </div> <!-- /row -->
        
        </footer> <!-- /footer -->


   <!-- Java Script
   ================================================== -->
   <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
   <script>window.jQuery || document.write('<script src="js/jquery-1.10.2.min.js"><\/script>')</script>
   <script type="text/javascript" src="js/jquery-migrate-1.2.1.min.js"></script>   
   <script src="js/jquery.flexslider.js"></script>
   <script src="js/jquery.fittext.js"></script>
   <script src="js/backstretch.js"></script> 
   <script src="js/waypoints.js"></script>  
   <script src="js/main.js"></script>
   <script id="dsq-count-scr" src="//marinoblog.disqus.com/count.js" async></script>

</body>

</html>