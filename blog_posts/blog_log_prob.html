<!DOCTYPE html>
<html lang="en">
<head>
<title>a note on evaluating probabilities</title>
<meta charset="utf-8">
<meta name="format-detection" content="telephone=no">
<link rel="icon" href="../images/favicon.ico">
<link rel="shortcut icon" href="images/favicon.ico">
<link rel="stylesheet" href="../css/stuck.css">
<link rel="stylesheet" href="../css/style.css">
<link rel="stylesheet" href="../css/ihover.css">
<script src="../js/jquery.js"></script>
<script src="../js/jquery-migrate-1.1.1.js"></script>
<script src="../js/script.js"></script>
<script src="../js/superfish.js"></script>
<script src="../js/jquery.equalheights.js"></script>
<script src="../js/jquery.mobilemenu.js"></script>
<script src="../js/jquery.easing.1.3.js"></script>
<script src="../js/tmStickUp.js"></script>
<script src="../js/jquery.ui.totop.js"></script>
<script>
 $(document).ready(function(){
  $().UItoTop({ easingType: 'easeOutQuart' });
  $('#stuck_container').tmStickUp({});
  });
</script>
<!--[if lt IE 9]>
 <div style=' clear: both; text-align:center; position: relative;'>
   <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
     <img src="http://storage.ie6countdown.com/assets/100/images/banners/warning_bar_0000_us.jpg" border="0" height="42" width="820" alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today." />
   </a>
</div>
<script src="js/html5shiv.js"></script>
<link rel="stylesheet" media="screen" href="css/ie.css">
<![endif]-->
<!--[if lt IE 10]>
<link rel="stylesheet" media="screen" href="css/ie1.css">
<![endif]-->

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});
    </script>
<script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

</head>
<body class="page1" id="top">
<!--==============================
              header
=================================-->
<header>
<!--==============================
            Stuck menu
=================================-->
  <section id="stuck_container">
    <div class="container">
      <div class="row">

          <div class="navigation ">
            <nav>
              <ul class="sf-menu">
               <li><a href="../index.html">about</a></li>
               <li><a href="../research.html">research</a></li>
               <li class="current"><a href="../blog.html">blog</a></li>
               <li><a href="../teaching.html">teaching</a></li>
             </ul>
            </nav>

          <div class="clear"></div>
          </div>
      </div>
    </div>
  </section>
</header>
<!--=====================
          Content
======================-->
<section class="content"><div class="ic"></div>
  <div class="container">
    <div class="row">
      <div class="grid_8 preffix_2">
        <div class="ta__center">
          <h3>a note on evaluating probabilities</h3>
          <div class="st2">
          <p>
          This blog post covers the evaluation of probabilities for continuous variables. This is a basic concept that is covered in any introduction to statistics. Why write a blog post on this topic? Simply put, in combining <a href="https://en.wikipedia.org/wiki/Bayesian_probability">Bayesian</a> probability theory with deep learning, I often witness a great deal of confusion in evaluating probabilities for continuous variables. To help those entering this area, this blog post describes how to evaluate these probabilities.
          </p>
          <h5>
              probability
          </h5>
          <p>
          To start, let's first review the concept of <b>probability</b>. Under the Bayesian interpretation, probability, $p(X)$ expresses a degree of belief regarding the state, $X=x$, of some variable, $X$. Variables can take many different forms: binary, discrete, continuous, categorical, etc. over different spaces. For instance, a light switch can be represented as a binary variable that can take either an 'off' $(0)$ or 'on' $(1)$ state, $X \in \{0, 1 \}$. If we add a dimmer to the light switch, we could instead represent the output as a continuous variable over an interval from minimum to maximum brightness, $X \in [ x_\text{min}, x_\text{max} ]$.
          
          </p>
          <p>
          Before observing the state of the light switch, we can express our belief about its state using probability. With the binary light switch, we might say that there is a probability of $0.9$ that the switch is in the 'off' state. Mathematically, we assign a <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution"><em>Bernoulli</em></a> probability distribution to the variable, with
          \begin{equation}
          P(X=0) = 0.9 \nonumber.
          \end{equation}
          
          Because probability distributions must sum to $1$ (the variable must be in some state), we therefore also express the belief that the light is 'on' with probability
          \begin{equation}
          P(X=1) = 1 - 0.9 = 0.1. \nonumber
          \end{equation}
          We have assigned a <b>probability mass</b> of $0.9$ to 'off' and $0.1$ to 'on'. As we see, probability distributions are just functions that assign a certain amount of mass to each state of the variable.
          </p>
          <center><img src="../images/blog_images/blog_log_prob/compare_distributions.png" alt="" /></center>
          <p>
          What happens when we swap out our binary light switch for the dimmer switch? The state of the dimmer can be anywhere in the interval from $x_{min}$ to $x_{max}$. We're confronted with an issue: there are an <em>infinite</em> number of continuous states in this interval, so how do we assign a probability mass to any given state? The short answer is that we cannot, but the situation is not as bad as it seems. The key is to realize the following: even though an underlying variable might take an infinite number of possible states, <em>our actual observations are never infinitely precise</em>. This arises from limits in our observation capabilities (e.g. the sensitivity of your retina, computer memory is discrete) and possibly inherent quantization (e.g. photons of light). Continuous variables are always observed with uncertainty!
          </p>
          <p>
          So how do we model the probability distribution over the states of a continuous variable? One option is to abandon the notion of a continuous variable entirely and instead use a categorical variable. That is, we break up the observation interval into a set of bins that correspond to our observation uncertainty windows. Then, using a <a href="https://en.wikipedia.org/wiki/Multinomial_distribution"><em>multinomial</em></a> distribution, we assign a probability mass to each of these bins:
          
          \begin{equation}
          P(X=x_\ell) = P_\ell \text{  for } \ell=1, \dots, L, \nonumber
          \end{equation}
          
          where here we have $L$ bins, with $P_\ell$ the probability for bin $\ell$, and $\sum_\ell P_\ell = 1$. If our observations always fall into the same set of bins, e.g. RGB pixels take integer values from 0 to 255, then this can be an effective way to model an underlying continuous variable. However, there are drawbacks to this method. When our uncertainty bins are not easily partitioned (e.g. the sensitivity of the human retina varies with light intensity) or if we have a large number of bins, this approach will not work. Likewise, this approach neglects the relational nature of the states, i.e. states are either larger or smaller than others. A multinomial distribution would work just as well if the bins were arranged randomly.
          </p>
          <p>
          Perhaps a more elegant way to model the probability over the states of a continuous variable is through a <b>probability density</b> function, $p(X)$. Note that we're using upper case $P$ for probability distributions and lower case $p$ for probability densities. Rather than assigning a mass to each state, a probability density function specifies a density that varies over states. To find the mass over an observation uncertainty window, we then <em>integrate</em> the density over this window. As in the case of probability distributions, we ensure that the probability density must integrate to $1$ because, again, the variable must be in some state:
          
          \begin{equation}
          \int p (X) dX = 1. \nonumber
          \end{equation}
          </p>
          <p>
          Going back to our dimmer light switch, we could, for example, model the brightness from the dimmer switch as a <a href="https://en.wikipedia.org/wiki/Normal_distribution"><em>Gaussian</em></a> density, $p(X) = \mathcal{N} (X; \mu, \sigma^2)$, specified by a mean, $\mu$, and a variance, $\sigma^2$ (although a <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta</a> density would be more appropriate). The mean is the center of our estimate, and the variance controls the width of that estimate. When we make an observation of the brightness resulting from the dimmer switch, we then take our Gaussian density and integrate over our observation window to find the probability mass assigned to that observation. If our observation is in the window $x_\text{obs} = [ x_\text{obs min}, x_\text{obs max} ]$, then we calculate
          
          \begin{equation}
          P(x_\text{obs}) = \int_{x_\text{obs min}}^{x_\text{obs max}} \mathcal{N} (X; \mu, \sigma^2) dX. \nonumber
          \end{equation}
          </p>
          <center><img src="../images/blog_images/blog_log_prob/Gaussian_integration.png" alt="" /></center>
          <p>
          Equivalently, we can use the density's corresponding <a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function">cumulative distribution function (CDF)</a>, which quantifies the amount of probability mass under the curve from $-\infty$ to each point. Calculating the probability mass within a window then becomes a calculation of the difference between the CDF at the edges of the window:
          $$
          P(x_\text{obs}) = \text{CDF} (x_\text{obs max}) - \text{CDF} (x_\text{obs min}).
          $$
          </p>
          <h5>
              latent variable models + variational inference
          </h5>
          <p>
          We're now going to apply these ideas to an important area of machine learning: generative models. In particular, we're going to discuss <b>latent variable models</b>. A latent variable model is a <em>probabilistic graphical model</em> that models an observed variable, $X$, as being generated from an underlying latent variable, $Z$. This is done using parameters, $\theta$, that define a joint probability,
          
          \begin{equation}
          p_\theta (X, Z) = p_\theta (X | Z) p_\theta (Z). \nonumber
          \end{equation}
          
          The joint distribution quantifies the  co-occurrence probability of each value of the latent variable with each value of the observed variable. It is important to note that the exact forms of $X$ and $Z$ are distinct; either variable can be binary, discrete, continuous, etc., depending on the data and the assumptions of the model. Note that I have used the lower case $p$ here, however, whether these are distributions or densities depends on the forms of the variables.
          </p>
          <p>
          Learning the model parameters, $\theta$, or inferring the posterior distribution over the latent variable from an observation, $p (Z | X)$, are both computationally intractable for most model classes. This arises from the fact that both tasks require evaluating the (marginal) log-likelihood:
          
          \begin{equation}
          p_\theta (X) = \int p_\theta (X, Z) dZ, \nonumber
          \end{equation}
          
          which involves integrating over the latent variable. With a continuous latent variable or a complicated model, this is too computationlly costly. For this reason, we often resort to <em>approximate</em> inference methods. <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">Variational inference</a> introduces an approximate posterior, $q(Z | X)$, that induces a lower bound on the marginal log-likelihood, often referred to as the evidence lower bound (ELBO):
          
          \begin{equation}
          \mathcal{L} = \mathbb{E}_{Z \sim q(Z | X)} \left[ \log p_\theta (X, Z) - \log q (Z | X) \right] \leq \log p_\theta (X). \nonumber
          \end{equation}
          
          Inference and learning then become alternating optimization problems. During inference, we optimize $\mathcal{L}$ w.r.t. $q (Z | X)$. During learning, we optimize $\mathcal{L}$ w.r.t. $\theta$. This is referred to as the <em>variational EM algorithm</em>. We can equivalently write the ELBO as
          
          \begin{equation}
          \mathcal{L} = \mathbb{E}_{Z \sim q(Z | X)} \left[ \log p_\theta (X | Z) \right] - D_{KL} (q (Z | X) || p_\theta (Z)), \nonumber
          \end{equation}
          
          where $D_{KL}$ denotes the <a href="https://en.wikipedia.org/wiki/Kullback–Leibler_divergence">KL-divergence</a> between the two distributions within the parentheses. The first term in this expression, the conditional log-likelihood, quantifies how well the model fits (reconstructs) the observation. The second term quantifies the agreement between the approximate posterior and the prior, which can be considered as a form of regularization.
          </p>
          <p>
          Let's step back for one moment to assess the ELBO. For any given observation, the ELBO is always less than or equal to the marginal log-likelihood. Note that since $p_\theta (X)$ is a probability, evaluating it at the observation (or observation window) should result in a number between 0 and 1. Thus, the quantity $\log p_\theta (X)$, and therefore $\mathcal{L}$, should always be less than or equal to 0. In fact, this will be less than or equal to the negative <em>information</em> contained in the observation, as evaluated using the true generating probability distribution. In other words, the best that our model can possibly do is to exactly recover the true data generating distribution.
          </p>
          <p>
          One popular model class is that of latent Gaussian models, which model continuous latent variables using Gaussian densities, i.e. $p_\theta (Z) = \mathcal{N} (Z; \mu_p, \sigma_p^2)$. Those within the deep learning community often consider these models as being interchangeable with <a href="https://arxiv.org/abs/1312.6114">variational auto-encoders (VAEs)</a>, but, in fact, a VAE is just one particular method of training a latent Gaussian model using variational inference. Specifically, VAEs use an <em>inference model</em> to output estimates of $q (Z | X)$, rather than performing exact inference optimization. Although this leads to <a href="https://arxiv.org/abs/1801.03558">sub-optimal</a> model performance, it's incredibly fast to train.
          </p>
          <p>
          Again, note that although $p_\theta (Z)$ is modeled using a Gaussian, $p_\theta (X | Z)$ can take any form that can model the observations. The original papers that introduced the VAE (<a href="https://arxiv.org/abs/1312.6114">here</a> and <a href="https://arxiv.org/abs/1401.4082">here</a>) both presented quantitative results using Bernoulli output distributions on a binarized version of the MNIST data set. Evaluating $\log p_\theta (X | Z)$ is relatively straightforward in this case: we simply evaluate how much probability mass the model placed at the observed binary values. The papers also present results using Gaussian output densities on natural image datasets. However, the reported results of the ELBO from the <a href="https://arxiv.org/abs/1312.6114">AEVB</a> paper are positive, with higher values being reported as better. Most likely, the reported values were estimated by evaluating the conditional likelihood, a Gaussian density, at a single value, rather than over a pixel bin width.
          </p>
          <p>
          Since the original VAE papers, a number of papers have come along with improvements for generation, inference, or both. For instance, <a href="https://arxiv.org/abs/1502.04623">DRAW</a> uses recurrent networks in the inference and generative models. The <a href="https://arxiv.org/abs/1602.02282">Ladder VAE</a> adds a "top-down" component to the inference model. Many works have also attempted to bring VAE-type models to the sequential setting, such as <a href="https://arxiv.org/abs/1506.02216">VRNN</a>, <a href="https://arxiv.org/abs/1605.07571">SRNN</a>, etc., where speech, music, and handwriting datasets have been modeled. A number of works have also looked at using similar models for video data, such as <a href="https://arxiv.org/abs/1605.06432">DVBF</a>, <a href="https://arxiv.org/abs/1710.05741">KVAE</a>, <a href="https://arxiv.org/abs/1802.07687">SVG</a>, etc. Unfortunately, many papers in this area gloss over the details when evaluating on continuous observations. DRAW, as well as SVG, use mean squared error instead of evaluating the conditional likelihood when training on natural images. Of the works that use Gaussian output densities, many, such as VRNN, SRNN, Ladder VAE, etc., evaluate the log density at a single point, rather than integrating the density over the observation window. Like the AEVB paper, they incorrectly report positive log-likelihood values. This is also reflected in open source <a href="https://github.com/kuleshov/convolutional-draw">implementations</a> online.
          </p>
          <p>
           I don't mean to disparage or single out these authors or their works. Each of them has made meaningful research contributions. Yet, it is important that we be absolutely clear and precise when we implement models and report results. If you're not evaluating the conditional likelihood, then explain why you feel this is a reasonable choice. If you're reporting log-likelihood (or ELBO) estimates, make sure they're sensible and evaluated correctly. If you're reporting log density, make sure this is clear and well-justified. We cannot just abandon theory or best practices whenever it suits us. That hinders our ability to accurately compare methods, creating confusion and limiting our progress as a field. As a research community, we must hold each other to high standards and avoid being swept up in technical jargon and hand-waving.
          </p>
          <h5>
              modeling continuous observations
          </h5>
          <p>
          To close, let's walk through the math and code for how to evaluate probabilities from probability densities over continuous variables. As our example, we'll use the Gaussian density, but the same rules apply to other probability densities. The probability density function for a univariate Gaussian is expressed mathematically as:
          
          \begin{equation}
          \mathcal{N} (X; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left( -\frac{(X-\mu)^2}{2 \sigma^2} \right) \nonumber
          \end{equation}
          
          Most of the deep learning libraries provide all of the necessary primitives to implement this density, or more commonly, its logarithm. Many libraries, including Tensorflow and PyTorch, have even moved in the direction of having all of these probabilistic programming functionalities built directly into the library. For instance, in PyTorch, one can express the log density as:
          </p>
          <script src="https://gist.github.com/joelouismarino/eb81848b5616de71ed06cba9dc36770a.js"></script>
          <p>
          Again, to evaluate the probability mass within a window, we integrate the density over that window or use the CDF. The CDF for a Gaussian density takes the following form:
          
          \begin{equation}
          \Phi (X) = \frac{1}{2} \left[ 1 + \text{erf} \left( \frac{X - \mu}{\sqrt{2 \sigma^2}} \right) \right], \nonumber
          \end{equation}
          
          where $\text{erf}$ denotes the <a href="https://en.wikipedia.org/wiki/Error_function">error function</a>. Again, most deep learning libraries provide the primitives to implement this from scratch, or even provide it directly. In PyTorch, this is expressed as:
          </p>
          <script src="https://gist.github.com/joelouismarino/621ac2b8137688f40697ad41808ab4b7.js"></script>
          <p>
          So, if we want to evaluate, for example, the probability mass under a Gaussian density in one pixel, we need to evaluate the CDF at each end of the pixel window. Often, to make the optimization easier, we rescale images to the interval $[0, 1]$, so the pixel width is effectively scaled to $\frac{1}{256}$. Thus, computing the probability mass of an image, $x_\text{image}$, is as straightforward as calculating the CDF at $x_\text{image} + \frac{1}{256}$ and subtracting the result of the CDF at $x_\text{image}$:
          
          \begin{equation}
          P(x_\text{image}) = \text{CDF} (x_\text{image} + \frac{1}{256}) - \text{CDF} (x_\text{image}). \nonumber
          \end{equation}
          
          And with that, we can evaluate the probability mass assigned by our model.
          </p>
          <h5>
              some extra notes
          </h5>
          <p>
          The auto-regressive generative modeling community often uses multinomial outputs to model images. See, for example, <a href="https://arxiv.org/abs/1601.06759">Pixel RNN</a> for a discussion of this approach. The <a href="https://arxiv.org/abs/1604.08772">Convolutional DRAW</a> paper uses a different technique for evaluating Gaussian log-likelihoods, based on comparing the ratio between the density and a uniform density. The <a href="https://arxiv.org/abs/1701.05517">Pixel CNN++</a> paper uses a discretized Logistic density to model images. However, perhaps the most exciting development in the area of density estimation is the use of invertible transforms that allow one learn intermediate spaces where it is easier to model the data distribution. The main example of this is <a href="https://arxiv.org/abs/1605.08803">Real NVP</a>, but the same idea is used in <a href="https://arxiv.org/abs/1505.05770">Normalizing Flows</a>. See Eric Jang's <a href="https://blog.evjang.com/2018/01/nf1.html">blog post</a> for a nice discussion of this technique.
          </p>
        </div>
        </div>
      </div>
    </div>
  </div>


</section>

</body>
</html>
