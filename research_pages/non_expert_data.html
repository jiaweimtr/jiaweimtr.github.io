<!DOCTYPE html>
<!--[if lt IE 8 ]><html class="no-js ie ie7" lang="en"> <![endif]-->
<!--[if IE 8 ]><html class="no-js ie ie8" lang="en"> <![endif]-->
<!--[if IE 9 ]><html class="no-js ie ie9" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!--><html class="no-js" lang="en"> <!--<![endif]-->
<head>

   <!--- Basic Page Needs
   ================================================== -->
   <meta charset="utf-8">
	<title>Research</title>
	<meta name="description" content="">  
	<meta name="author" content="">

   <!-- Mobile Specific Metas
   ================================================== -->
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<!-- CSS
    ================================================== -->
   <link rel="stylesheet" href="css/base.css">
	<link rel="stylesheet" href="css/main.css">
   <link rel="stylesheet" href="css/media-queries.css">         

   <!-- Script
   =================================================== -->
	<script src="js/modernizr.js"></script>

   <!-- Favicons
	=================================================== -->
	<link rel="shortcut icon" href="favicon.png" >

</head>

<body>

   <div id="top"></div>
   <div id="preloader"> 
	   <div id="status">
         <img src="images/loader.gif" height="60" width="60" alt="">
         <div class="loader">Loading...</div>
      </div>
   </div>

   <!-- Header
   =================================================== -->
   <header id="main-header">

   	<div class="row header-inner">

	      <div class="logo">
              <a class="smoothscroll" </a>
	      </div>

	      <nav id="nav-wrap">         
	         
	         <a class="mobile-btn" href="#nav-wrap" title="Show navigation">
	         	<span class='menu-text'>Show Menu</span>
	         	<span class="menu-icon"></span>
	         </a>
         	<a class="mobile-btn" href="#" title="Hide navigation">
         		<span class='menu-text'>Hide Menu</span>
         		<span class="menu-icon"></span>
         	</a>         
            
	         <ul id="nav" class="nav">
                <li><a href="index.html#about">About</a></li>
                <li class="current"><a href="index.html#portfolio">Research</a></li>
	            <li><a href="index.html#journal">Blog</a></li>
	            <li><a href="index.html#contact">Contact</a></li>
	         </ul> 

	      </nav> <!-- /nav-wrap -->	      

	   </div> <!-- /header-inner -->

   </header>

   <!-- Content
   ================================================== -->
   <section id="content">

   	<div class="row portfolio-content">

	   	<div class="entry tab-whole nine columns centered">

	         <header class="entry-header">

					<h1 class="entry-title">
							Non-expert Labels Improve Fine-Grained Object Recognition
					</h1>
						 
				</header>

				<div class="entry-content">
                    
                    <h3> Motivation </h3>
                    
					<p>
                    In fine-grained object recognition, data examples, while often still plentiful, are typically too difficult for Amazon Mechanical Turk annotators to accurately label. Untrained annotators are unfamiliar with class names and lack the expertise to distinguish between fine-grained classes. Likewise, in many areas, there are too few experts or enthusiasts to create a sufficiently large dataset.
                    </p>
                    
                    <center>
                        <img src="../images/research_images/non_expert_data/dogs_birds.png">
                            <p>
                            <em>Example fine-grained object domains: birds and dogs.</em>
                            </p>
                    </center>
                    
                    <p>
                    Fine-grained object classes, by definition, share many visual features, allowing us to group these classes into coarse clusters, which will tend to form a visual taxonomy. For instance, all birds share certain visual features, all birds of prey share a more specific set of visual features, and all hawks share an even more specific set of visual features. Thus, we can transform the overall task into a series of coarse- to fine-grained recognition tasks. Coarse splittings near the root of the taxonomy can be easily distinguished by non-experts, whereas correctly distinguishing between branches near the leaf level requires extensive domain knowledge. Annotators can now provide class label annotations at whichever level of the taxonomy they feel most confident making a classification. While not as informative as fine-grained labels, non-expert labels nevertheless contain visually relevant information and may be far easier to collect. In this way, one can collect a dataset in which non-experts provide labels for a majority of the examples, with experts labeling only a small subset. By training on both expert and non-expert labels, one can substantially improve performance over using expert labels alone.
                    </p>
                    
                    <h3> Method </h3>
                    
                    <p>
                    Given a fine-grained object domain, we start by constructing a visual taxonomy. We do this manually to ensure that coarse classes within the taxonomy are visually interpretable. Our approach consists of grouping object classes first by overall shape and form, then color, and finally by specific traits. This is shown in the following figure.
                    </p>
                    
                    <center>
                        <img src="../images/research_images/non_expert_data/bird_taxonomy.png">
                            <p>
                            <em>Taxonomy construction for the North American birds domain.</em>
                            </p>
                    </center>
                    
                    <p>
                    There are a number of methods for taxonomic training. We use <a href="tax_curriculum_learning.html">taxonomic curriculum learning </a>, in which a classifier is trained on classes in a coarse-to-fine manner. To accomodate this approach, we make a series of cuts through the taxonomy. These cuts define the levels of the taxonomy that we use for training. We then train the classifier, a convolutional neural network, on successively deeper cuts through the taxonomy.
                    </p>
                    
                    <center>
                        <img src="../images/research_images/non_expert_data/tax_cur_diagram.png">
                            <p>
                            <em>Outline of taxonomic curriculum learning approach.</em>
                            </p>
                    </center>
                    
                    <h3> Results </h3>
                    
                    <p>
                    We demonstrate our methods using two fine-grained datasets. The first is <a href="http://vision.stanford.edu/aditya86/ImageNetDogs/">Stanford Dogs</a>, which contains 20,580 images of 120 classes of dogs. Each class contains 100 training images of dogs at various ages and colors. The other dataset is a collection of 76,613 images of 555 classes of North American birds compiled from <a href="http://dl.allaboutbirds.org/nabirds">NABirds</a>, <a href="http://www.vision.caltech.edu/visipedia/CUB-200-2011.html">CUB-200-2011</a>, and <a href="http://birdsnap.com">Birdsnap</a>. We constructed visual taxonomies for both of these domains. The dog domain has three levels, with 35, 72, and 120 classes at each respective level. The birds taxonomy has four levels, with 17, 130, 241, and 555 classes at each respective level.
                    </p>
                    
                    <p>
                    The size of the birds dataset is atypical of the amount of data available for many real world fine-grained domains. A researcher creating a vision system for an obscure domain may only have access to a handful of expert labeled images per class. For this reason, we selected a subset containing 4,995 images (9 images per class) from our 52,701 training images as our more 'realistic' dataset. We trained in three different settings: 4,995 expert labeled examples, 52,701 expert labeled examples, and a partitioning of our dataset into 47,706 non-expert labeled images and 4,995 expert labeled images. Non-expert labeled images have coarse labels at the second level of the taxonomy, whereas expert labeled images have fine-grained labels at the fourth taxonomic level. Each model was trained by re-initializing the loss layers in a pre-trained <a href="http://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf">GoogLeNet</a> network and fine-tuning the entire network.
                    </p>
                    
                    <p>
                    The network trained just on the subset of expert labeled data reached an accuracy of <b> 41.8% </b>, whereas the network trained with the additional 47,706 non-expert labeled images reached an accuracy of <b> 47.9% </b>. Training with all 52,701 expert labeled images resulted in an accuracy of <b> 69.1% </b>. All accuracies are measured at the fine-grained level of the taxonomy. We see that, while we cannot entirely make up for a lack of expert labels, supplementing a small set of expert labeled data with a much larger set of non-expert labeled data can result in a substantial improvement in performance.
                    </p>
                    
                    <center>
                        <img src="../images/research_images/non_expert_data/dogs_non_expert.png">
                            <p>
                            <em>Performance improvements from training with non-expert data. Total number of images held constant.</em>
                            </p>
                    </center>
                    
                    <p>
                    With the Stanford Dogs dataset, we selected subsets of 5, 10, and 50 images from the 100 images for each class. These subsets are our expert labeled training sets, with labels at level 3 of the taxonomy. We trained models using all 100 images with non-expert labels at either the first or second level of the taxonomy in combination with each of the subsets. For comparison, we also trained models using only the expert labeled subsets. The fine-grained accuracy of these models is plotted in the following figure. Increasing the ratio of non-expert labels to expert labels leads to a larger gain in performance. The plot also portrays the relative information contained in the labels at each level. For a fixed number of expert labeled examples, training with non-expert labels at level 2 is better than training with the same amount of non-expert labels at level 1, as should be expected.  However, training with only 10 expert labels per class outperforms training with 5 expert labels and 95 level 2 non-expert labels per class.
                    </p>
                    
                    <h3> Conclusion </h3>
                    
                    <p>
                    Visual taxonomies allow for the possibility of using non-expert labels for fine-grained object recognition. These labels are relatively easy to collect, and when combined with a small number of expert labels, can result in significant performance gains. With the domains that we analyzed, we found that for a given amount of additional expert labeled data, one typically requires an order of magnitude more non-expert labeled data to achieve the same performance improvement. Of course, there may be a limit to this improvement. Our method is not confined to taxonomic tree structures; other coarse clusterings of classes could work as well. For object domains like food, this may be a more representative structure.

                    </p>
                    
				</div>
                
                <div id="disqus_thread"></div>
                <script>
                    /**
                     * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
                     * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
                     */
                
                var disqus_config = function () {
                    this.page.url = 'http://joelouismarino.github.io/non_expert_data.html'; // Replace PAGE_URL with your page's canonical URL variable
                    this.page.identifier = non_expert_data; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
                };
                
                (function() { // DON'T EDIT BELOW THIS LINE
                 var d = document, s = d.createElement('script');
                 
                 s.src = '//marinoblog.disqus.com/embed.js';
                 
                 s.setAttribute('data-timestamp', +new Date());
                 (d.head || d.body).appendChild(s);
                 })();
                    </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
	         
	      </div> <!-- /entry -->

	   </div> <!-- /portfolio-content -->
	   

   </section> <!-- /content -->  


<!-- Footer
 ================================================== -->
<footer>
    
    <div class="row">
        
        <div class="twelve columns tab-whole right-cols">
            
            <div class="row">
                
                <div class="columns">
                    <h3 class="address">Contact</h3>
                    <p>
                    jmarino [at] caltech.edu
                    </p>
                    <p>
                    1200 E. California Blvd.<br>
                    MC 136-93<br>
                    Pasadena, 91125 CA<br>
                    </p>
                </div> <!-- /columns -->
                
                <div class="columns">
                    <h3 class="contact">Connect</h3>
                    
                    <ul>
                        <li><a href="https://www.linkedin.com/pub/joe-marino/37/bb0/73"><span></span><i class="fa fa-linkedin"></i></a>
                            <a href="https://github.com/joelouismarino"><span></span><i class="fa fa-github"></i></a>
                            <a href="https://www.facebook.com/joe.marino.9022"><span></span><i class="fa fa-facebook"></i></a>
                        </li>
                    </ul>
                    
                </div> <!-- /Row(nested) -->
                
            </div>
            
            <p class="copyright">&copy; Copyright 2016 Joseph Marino.</a></p>
            
            <div id="go-top">
                <a class="smoothscroll" title="Back to Top" href="#content"><span>Top</span><i class="fa fa-long-arrow-up"></i></a>
            </div>
            
        </div> <!-- /row -->
        
        </footer> <!-- /footer -->


   <!-- Java Script
   ================================================== -->
   <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
   <script>window.jQuery || document.write('<script src="js/jquery-1.10.2.min.js"><\/script>')</script>
   <script type="text/javascript" src="js/jquery-migrate-1.2.1.min.js"></script>   
   <script src="js/jquery.flexslider.js"></script>
   <script src="js/jquery.fittext.js"></script>
   <script src="js/backstretch.js"></script> 
   <script src="js/waypoints.js"></script>  
   <script src="js/main.js"></script>

</body>

</html>