<!DOCTYPE html>
<!--[if lt IE 8 ]><html class="no-js ie ie7" lang="en"> <![endif]-->
<!--[if IE 8 ]><html class="no-js ie ie8" lang="en"> <![endif]-->
<!--[if IE 9 ]><html class="no-js ie ie9" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!--><html class="no-js" lang="en"> <!--<![endif]-->
<head>

   <!--- Basic Page Needs
   ================================================== -->
   <meta charset="utf-8">
	<title>Research</title>
	<meta name="description" content="">  
	<meta name="author" content="">
        
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    

   <!-- Mobile Specific Metas
   ================================================== -->
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<!-- CSS
    ================================================== -->
   <link rel="stylesheet" href="css/base.css">
	<link rel="stylesheet" href="css/main.css">
   <link rel="stylesheet" href="css/media-queries.css">         

   <!-- Script
   =================================================== -->
	<script src="js/modernizr.js"></script>

   <!-- Favicons
	=================================================== -->
	<link rel="shortcut icon" href="favicon.png" >

</head>

<body>

   <div id="top"></div>
   <div id="preloader"> 
	   <div id="status">
         <img src="images/loader.gif" height="60" width="60" alt="">
         <div class="loader">Loading...</div>
      </div>
   </div>

   <!-- Header
   =================================================== -->
   <header id="main-header">

   	<div class="row header-inner">

	      <div class="logo">
              <a class="smoothscroll" </a>
	      </div>

	      <nav id="nav-wrap">         
	         
	         <a class="mobile-btn" href="#nav-wrap" title="Show navigation">
	         	<span class='menu-text'>Show Menu</span>
	         	<span class="menu-icon"></span>
	         </a>
         	<a class="mobile-btn" href="#" title="Hide navigation">
         		<span class='menu-text'>Hide Menu</span>
         		<span class="menu-icon"></span>
         	</a>         
            
	         <ul id="nav" class="nav">
                <li><a href="index.html#about">About</a></li>
                <li class="current"><a href="index.html#portfolio">Research</a></li>
	            <li><a href="index.html#journal">Blog</a></li>
	            <li><a href="index.html#contact">Contact</a></li>
	         </ul> 

	      </nav> <!-- /nav-wrap -->	      

	   </div> <!-- /header-inner -->

   </header>

   <!-- Content
   ================================================== -->
   <section id="content">

   	<div class="row portfolio-content">

	   	<div class="entry tab-whole nine columns centered">

	         <header class="entry-header">

					<h1 class="entry-title">
							Taxonomy-Aware Decision-Theoretic Classifiers
					</h1>
						 
				</header>

				<div class="entry-content">
					<p class="lead">This research is currently ongoing.</p>
                    
                    
                    
                    <h3>What is a class taxonomy?</h3>

                    <p>
                        $\DeclareMathOperator*{\argmax}{arg\,max}$ $\DeclareMathOperator*{\argmin}{arg\,min}$
                        
                        In a classification setting, we have a particular data example $x$ (an image, a document, a user's online behavior, etc.) and we wish to choose the class $y$ that correctly classifies the example. We train a model to perform this classification, for each example making a prediction $\hat{y}$ that maximizes the posterior probability over some set of output classes $C$ ,

                        $$\hat{y}=\argmax_{y' \in C} P(Y=y'|X).$$
                        
                        <strong>However, the output classes rarely live in isolation.</strong> For instance, in the case of images, the class <em>car</em> is visually similar to the class <em>bus</em>. These classes share certain characteristics, such as windows, doors, wheels, and seats. Conversely, we can say that the class <em>car</em> is visually dissimilar to the class <em>apple</em>. These two classes share almost no characteristics. In cases in which we can define some measure of similarity between output classes, <strong>we can create a class taxonomy that contains the relationships between all classes.</strong> In this taxonomy, the level of similarity between two nodes is related to the distance between those nodes in the taxonomy. The set of output classes $C$ correspond to the leaf nodes of this taxonomy, with internal nodes corresponding to meta-classes that contain multiple leaf nodes. For instance, <em>car</em> and <em>bus</em> would both be descendants of the meta-class <em>vehicle</em>. An example of such a taxonomy is shown below.
                    </p>
				

                <center>
						<img width="500" height="460" alt="example taxonomy" src="images/example_taxonomy.png">
                </center>
                
                
                <p> We also see that <em>car</em>, <em>apple</em>, and <em>bus</em> are all descendants of the meta-class <em>physical object</em>. To traverse from one class in the taxonomy to another, the traversal path must pass through the closest common ancestor of those classes. Thus, consistent with our notion of similarity between these three classes, the taxonomic traversal distance from <em>car</em> to <em>apple</em> is greater than the taxonomic traversal distance from <em>car</em> to <em>bus</em>.
                </p>
                
                <h3>Why would we care about having a class taxonomy?</h3>
                <p>
                A class taxonomy can aid a classifier in both training and prediction. Namely, having a taxonomy allows us to
                
                <ul>
                    <li>train a classifier with preconceived notions of distances between classes. This can be especially useful in situations in which our feature representation of the data is inadequate or in a non-convex optimization setting. We can also output internal nodes when we are faced with uncertainty, allowing us to make more general predictions.</li>
                    <li>make more general class predictions with standard, non-taxonomically trained classifiers. In this way, we can ensure that our classifier's predictions maintain some desired level of precision.</li>
                </ul>
                
                </p>
                
                <h3>Taxonomic Training</h3>
                
                <p>
                One possible way in which to use a taxonomy to train a classifier is through utility maximization. We start by defining our goal of developing a <strong>decision-theoretic classifier $h(x)$ that maximizes the utility of the prediction on the training set</strong>:
                
                \begin{eqnarray}\mathcal{U}(h) = \sum_{(x,y)}\mu^y_{h(x)}.\end{eqnarray}
                
                Here, $\mu^y_{h(x)}$ is the utility of classifying an example $x$ as $h(x)$ when it belongs to ground truth class $y$. We can find these utilities using the taxonomy. For instance, we can define
                
                \begin{eqnarray}\mu^y_{h(x)} \equiv \exp{(-d^y_{h(x)})},\end{eqnarray}
                
                where $d^y_{h(x)}$ is the taxonomic traversal distance between class $y$ and class $h(x)$.
                
                If we have some scoring function $f$ in our classifier, then one form for our classifier's output could be
                
                \begin{eqnarray}h(x) = \argmax_{c} f_{c}(x),\end{eqnarray}
                
                where $f_{c}(x)$ is the value of the scoring function for class $c$ on data example $x$. In this method of training, the class $c$ can be any node in the taxonomy, both leaf nodes and internal nodes. If we let $c_x \equiv h(x)$ denote the prediction of the model for input $x$.  Then we can try to directly optimize utility:
                
                \begin{eqnarray}\argmax_{h} U(h) \equiv \argmax_{f} \sum_{(x,y)} \mu^y_{c_x}.\end{eqnarray}
                
                However, this quantity is flat or discontinuous everywhere w.r.t. $f$, and thus its gradient w.r.t. $f$ is 0 or undefined everywhere.  So we need to smooth our classifier's output somehow. One way of achieving this is by changing our classifier's output into a softmax:
                
                \begin{eqnarray}h(x,c) = \frac{\exp f_{c}(x)}{\sum_{c'} \exp f_{c'}(x)}.\end{eqnarray}
                
                <strong>This formulation does not model the probability of the each class, but rather the decision-theoretic softmax probability of predicting each class (given the utility matrix $\mu$ and scoring function $f$).</strong> The optimization expression for utility thus becomes
                
                \begin{eqnarray}\argmax_{f} U(h) = \argmax_{f} \sum_{(x,y)} \sum_c\mu^y_{c}\frac{\exp f_{c}(x)}{\sum_{c'}\exp f_{c'}(x)}.\end{eqnarray}
                
                This expression is differentiable, however it is non-convex. Thus, we can minimize the negative log-loss variant of this expression, giving
                
                \begin{eqnarray}\argmin_{f} \sum_{(x,y)} \sum_c-\mu^y_{c}\log\left(\frac{\exp f_{c}(x)}{\sum_{c'}\exp f_{c'}(x)}\right),
                \label{eqn:score_log}\end{eqnarray}
                
                which is a convex loss function, allowing us to train the classifier using gradient descent.
                </p>
                <p>
                The output of the classifier will be a node from the taxonomy, either a leaf or internal node. <strong> How do we specify our desired level of precision for our predictions? In other words, how do we specify when to predict an internal node and when to predict a leaf node?</strong> We can do this by modifying the utilities $\mu$ between classes, giving more weight to general or specific predictions. The utilities come from the taxonomic distances between classes in the taxonomy. We use a parameter $\alpha \in [0,2]$ to modify the distances between leaves and internal nodes in the taxonomy. $\alpha$ is defined to be the distance from a child node to its parent, as in the following figure.
                
                </p>
                
               <center>
                   <img width="300" height="213" alt="alpha tax" src="images/alpha_tax.png">
                </center>
               
               <p>
               To keep the distances between leaf nodes constant throughout the taxonomy, we also define the distance from a parent node to its child to be $2 - \alpha.$ <strong>When $\alpha = 0,$</strong> there is effectively no distance between each child and its parent, meaning there is a utility of $\exp(0) = 1$ between a leaf node and every internal node to which it belongs. In this case, <strong>the classifier is trained to always make general predictions.</strong> Conversely, <strong> when $\alpha = 2,$</strong> there is a large distance between each child node and its parent. In this situation, making general predictions carries very low utility, and <strong> the classifier is encouraged to always make specific predictions.</strong> By varying $\alpha,$ we can specify the level of precision with which we want to classify.
               </p>
               
               <p>
               <strong>In practice, how do we set $\alpha$?</strong> We can use two measures, taxonomic precision and taxonomic recall, to guide our decision. They are defined as follows:
               
               $$\text{Taxonomic Precision} = \sum_i \frac{|T_i \cap P_i|}{|P_i|}$$
               
               $$\text{Taxonomic Recall} = \sum_i \frac{|T_i \cap P_i|}{|T_i|}$$
               
               $T_i$ is the taxonomic path (from the root) of the correct class for example $x_i$, and $P_i$ is the taxonomic path of the predicted node for this example. Taxonomic precision is therefore measures whether we are making general enough predictions, and taxonomic recall measures whether we are making correct predictions. In making taxonomic predictions, our hope is to increase taxonomic precision (make more general, correct predictions when we are uncertain) while minimally reducing taxonomic recall (still make specific, correct decisions when we are fairly certain). We can train mutliple models with different values of $\alpha \in [0,2]$ and evaluate which model achieves some desired level of taxonomic precision while attaining the largest taxonomic recall.
               
               </p>
               
               <h3>Taxonomic Prediction</h3>
               
               <p>
               Training multiple models with taxonomic utility may be computationally infeasible. In these cases, it is still possible to use the taxonomy to make taxonomic predictions with a standard, non-taxonomic classifier. Again, we use the taxonomy and $\alpha$ to define a set of utilities $\mu$ between all classes (nodes) in the taxonomy. If our classifier gives us some probabalistic output over the leaf nodes, we can use a classifier of the form
               
               \begin{eqnarray}h(x) = \argmax_{c} \sum_{c'}\mu^{c'}_c P_{c'}(x)\end{eqnarray}
               
               to make taxonomic predictions. Here, $c$ is over all nodes in the taxonomy and $c'$ is over the leaf nodes (the ouputs of our non-taxonomic classifier). Again, varying $\alpha$ has a similar effect on the specificity of predictions, expressed in the following graphs:
               
               </p>

                <center>
                    <img width="700" height="286" alt="alpha graph" src="images/alpha_graphs.png">
                </center>

            </div>
	         
	      </div> <!-- /entry -->	      

	   </div> <!-- /portfolio-content -->
	   

   </section> <!-- /content -->  


<!-- Footer
 ================================================== -->
<footer>
    
    <div class="row">
        
        <div class="twelve columns tab-whole right-cols">
            
            <div class="row">
                
                <div class="columns">
                    <h3 class="address">Contact</h3>
                    <p>
                    jmarino [at] caltech.edu
                    </p>
                    <p>
                    1200 E. California Blvd.<br>
                    MC 136-93<br>
                    Pasadena, 91125 CA<br>
                    </p>
                    <p>
                    651.468.6441
                    </p>
                </div> <!-- /columns -->
                
                <div class="columns">
                    <h3 class="contact">Connect</h3>
                    
                    <ul>
                        <li><a href="https://www.linkedin.com/pub/joe-marino/37/bb0/73"><span></span><i class="fa fa-linkedin"></i></a>
                            <a href="https://github.com/joelouismarino"><span></span><i class="fa fa-github"></i></a>
                            <a href="https://www.facebook.com/joe.marino.9022"><span></span><i class="fa fa-facebook"></i></a>
                        </li>
                    </ul>
                    
                </div> <!-- /Row(nested) -->
                
            </div>
            
            <p class="copyright">&copy; Copyright 2015 Joseph Marino.</a></p>
            
            <div id="go-top">
                <a class="smoothscroll" title="Back to Top" href="#content"><span>Top</span><i class="fa fa-long-arrow-up"></i></a>
            </div>
            
        </div> <!-- /row -->
        
        </footer> <!-- /footer -->


   <!-- Java Script
   ================================================== -->
   <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
   <script>window.jQuery || document.write('<script src="js/jquery-1.10.2.min.js"><\/script>')</script>
   <script type="text/javascript" src="js/jquery-migrate-1.2.1.min.js"></script>   
   <script src="js/jquery.flexslider.js"></script>
   <script src="js/jquery.fittext.js"></script>
   <script src="js/backstretch.js"></script> 
   <script src="js/waypoints.js"></script>  
   <script src="js/main.js"></script>

</body>

</html>